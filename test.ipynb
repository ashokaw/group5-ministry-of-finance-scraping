{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8681d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.2\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (5.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.34.2)\n",
      "Requirement already satisfied: scrapy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (2.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (43.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (24.2.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: tldextract in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (5.1.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (2.0.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\dell\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy->-r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tldextract->scrapy->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tldextract->scrapy->-r requirements.txt (line 6)) (3.13.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcee4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "\n",
    "print(\"✅ All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d56da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83488009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and headers\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (https://github.com/your_repo_name)'\n",
    "}\n",
    "\n",
    "# Add a User-Agent to your requests to mimic a browser.\n",
    "# This is a key step in ethical scraping, as covered in Week 7[cite: 1158]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10248b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the page.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Check for bad status codes (4xx or 5xx) [cite: 518, 532]\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"Successfully fetched the page.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2ef37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming news headlines are inside <h2> tags with a specific class\n",
    "news_items = soup.find_all('h2', class_='news-title')\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each item to extract the text and a link\n",
    "for item in news_items:\n",
    "    title = item.get_text(strip=True)\n",
    "    link_tag = item.find('a')\n",
    "    if link_tag:\n",
    "        link = link_tag['href']\n",
    "        scraped_data.append({'Title': title, 'Link': url + link})\n",
    "\n",
    "# Convert to a DataFrame and display\n",
    "df = pd.DataFrame(scraped_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200cd084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 tables on the page.\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.08LKR  304.61LKR\n",
      "1      GBP  397.64LKR  410.70LKR\n"
     ]
    }
   ],
   "source": [
    "# Try to scrape any HTML tables from the page\n",
    "try:\n",
    "    tables = pd.read_html(url)\n",
    "    print(f\"Found {len(tables)} tables on the page.\")\n",
    "    # The first table is usually at index 0. You can inspect others if needed.\n",
    "    df_table = tables[0]\n",
    "    print(df_table.head())\n",
    "except Exception as e:\n",
    "    print(f\"No tables found or an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585097bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x0x7ff7bbc7e415+77285]\n\tGetHandleVerifier [0x0x7ff7bbc7e470+77376]\n\t(No symbol) [0x0x7ff7bba49a6a]\n\t(No symbol) [0x0x7ff7bbaa0406]\n\t(No symbol) [0x0x7ff7bbaa06bc]\n\t(No symbol) [0x0x7ff7bbaf3ac7]\n\t(No symbol) [0x0x7ff7bbac864f]\n\t(No symbol) [0x0x7ff7bbaf087f]\n\t(No symbol) [0x0x7ff7bbac83e3]\n\t(No symbol) [0x0x7ff7bba91521]\n\t(No symbol) [0x0x7ff7bba922b3]\n\tGetHandleVerifier [0x0x7ff7bbf61efd+3107021]\n\tGetHandleVerifier [0x0x7ff7bbf5c29d+3083373]\n\tGetHandleVerifier [0x0x7ff7bbf7bedd+3213485]\n\tGetHandleVerifier [0x0x7ff7bbc9884e+184862]\n\tGetHandleVerifier [0x0x7ff7bbca055f+216879]\n\tGetHandleVerifier [0x0x7ff7bbc87084+113236]\n\tGetHandleVerifier [0x0x7ff7bbc87239+113673]\n\tGetHandleVerifier [0x0x7ff7bbc6e298+11368]\n\tBaseThreadInitThunk [0x0x7ffa8df5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8f69c34c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Wait for up to 10 seconds for the dynamic content (e.g., a table with the ID 'data-table') to be visible [cite: 1053]\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     14\u001b[0m     EC\u001b[38;5;241m.\u001b[39mvisibility_of_element_located((By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-table\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Now that the element has loaded, you can get the page source\u001b[39;00m\n\u001b[0;32m     18\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:138\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x0x7ff7bbc7e415+77285]\n\tGetHandleVerifier [0x0x7ff7bbc7e470+77376]\n\t(No symbol) [0x0x7ff7bba49a6a]\n\t(No symbol) [0x0x7ff7bbaa0406]\n\t(No symbol) [0x0x7ff7bbaa06bc]\n\t(No symbol) [0x0x7ff7bbaf3ac7]\n\t(No symbol) [0x0x7ff7bbac864f]\n\t(No symbol) [0x0x7ff7bbaf087f]\n\t(No symbol) [0x0x7ff7bbac83e3]\n\t(No symbol) [0x0x7ff7bba91521]\n\t(No symbol) [0x0x7ff7bba922b3]\n\tGetHandleVerifier [0x0x7ff7bbf61efd+3107021]\n\tGetHandleVerifier [0x0x7ff7bbf5c29d+3083373]\n\tGetHandleVerifier [0x0x7ff7bbf7bedd+3213485]\n\tGetHandleVerifier [0x0x7ff7bbc9884e+184862]\n\tGetHandleVerifier [0x0x7ff7bbca055f+216879]\n\tGetHandleVerifier [0x0x7ff7bbc87084+113236]\n\tGetHandleVerifier [0x0x7ff7bbc87239+113673]\n\tGetHandleVerifier [0x0x7ff7bbc6e298+11368]\n\tBaseThreadInitThunk [0x0x7ffa8df5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8f69c34c+44]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the driver (make sure the chromedriver is in your path or folder)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for up to 10 seconds for the dynamic content (e.g., a table with the ID 'data-table') to be visible [cite: 1053]\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.ID, 'data-table'))\n",
    "    )\n",
    "\n",
    "    # Now that the element has loaded, you can get the page source\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Scrape the dynamic content from the soup object\n",
    "    dynamic_data_element = soup.find('table', id='data-table')\n",
    "\n",
    "    # Now, you can process the dynamic_data_element with BeautifulSoup\n",
    "    # ... your scraping logic here ...\n",
    "\n",
    "finally:\n",
    "    # Always remember to close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2884bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to data/scraped_treasury_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the scraped data to a CSV file\n",
    "df.to_csv('data/scraped_treasury_data.csv', index=False)\n",
    "print(\"Data saved successfully to data/scraped_treasury_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133460f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.treasury.gov.lk/\"\n",
    "tables = pd.read_html(url)  # returns list of DataFrames\n",
    "df = tables[0]  # first table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318c45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 tables\n",
      "\n",
      "Table 0 shape: (2, 3)\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.08LKR  304.61LKR\n",
      "1      GBP  397.64LKR  410.70LKR\n",
      "\n",
      "Table 1 shape: (3, 3)\n",
      "                    Unnamed: 0   Year / Month Amount / LKR Bn\n",
      "0  Governement Revenue & Grant  Jan-July 2024          2155.9\n",
      "1       Government Expenditure  Jan-July 2024          3034.4\n",
      "2       Overall Budget Deficit              .               .\n",
      "\n",
      "Table 2 shape: (2, 4)\n",
      "   #               Item   Pettah Dambulla\n",
      "0  1              Samba  0LKR/Kg  0LKR/Kg\n",
      "1  2  Red-Onions(Local)  0LKR/Kg  0LKR/Kg\n",
      "\n",
      "Table 3 shape: (2, 4)\n",
      "     Month/Year  Export  Import  Trade Balance\n",
      "0  Jan-Nov 2023       0       0              0\n",
      "1  Jan-Nov 2022       0       0              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.treasury.gov.lk/\"\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\nTable {i} shape: {table.shape}\")\n",
    "    print(table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862d8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tables[2]  # replace with correct index\n",
    "df.to_csv(\"treasury_data_raw.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3383e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folders created: ['data_raw', 'data_clean', 'slides', 'docs']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"data_raw\", \"data_clean\", \"slides\", \"docs\"]\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Project folders created:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5554d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"Test notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d90842",
   "metadata": {},
   "source": [
    "Tute 1 excersises for MOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56e8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Status: 200\n",
      "Found 4 tables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Currency</th>\n",
       "      <th>Buying</th>\n",
       "      <th>Selling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USD</td>\n",
       "      <td>297.35LKR</td>\n",
       "      <td>304.87LKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBP</td>\n",
       "      <td>401.53LKR</td>\n",
       "      <td>414.18LKR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Currency     Buying    Selling\n",
       "0      USD  297.35LKR  304.87LKR\n",
       "1      GBP  401.53LKR  414.18LKR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Page title: Ministry of Finance - Sri lanka\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Test URL\n",
    "url = \"https://www.treasury.gov.lk/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"✅ Status:\", response.status_code)\n",
    "\n",
    "# Try reading tables\n",
    "try:\n",
    "    tables = pd.read_html(url)\n",
    "    print(f\"Found {len(tables)} tables\")\n",
    "    display(tables[0].head())\n",
    "except Exception as e:\n",
    "    print(\"No HTML tables found:\", e)\n",
    "\n",
    "# Try parsing with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(\"✅ Page title:\", soup.title.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070744",
   "metadata": {},
   "source": [
    "Get all links on homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9333bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links found: 96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['#home',\n",
       " '#budget-highlights',\n",
       " '#at-a-glance',\n",
       " '#mof-links',\n",
       " '/',\n",
       " '/si/#',\n",
       " '/ta/#',\n",
       " '/#',\n",
       " '/search',\n",
       " '/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.treasury.gov.lk/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract all links\n",
    "links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "print(\"Total links found:\", len(links))\n",
    "\n",
    "# Show first 10\n",
    "links[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f257832",
   "metadata": {},
   "source": [
    "Useful links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27956f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid links found: 92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.treasury.gov.lk/',\n",
       " 'https://www.treasury.gov.lk/si/#',\n",
       " 'https://www.treasury.gov.lk/ta/#',\n",
       " 'https://www.treasury.gov.lk/#',\n",
       " 'https://www.treasury.gov.lk/search',\n",
       " 'https://www.treasury.gov.lk/',\n",
       " 'https://www.treasury.gov.lk/web/about',\n",
       " 'https://www.treasury.gov.lk/web/about/section/purview%20and%20function',\n",
       " 'https://www.treasury.gov.lk/web/about/section/minister%20of%20finance',\n",
       " 'https://www.treasury.gov.lk/web/about/section/deputy%20ministers']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "\n",
    "# Keep only proper links (no '#' anchors)\n",
    "valid_links = [link for link in links if link and not link.startswith(\"#\")]\n",
    "\n",
    "# Convert relative URLs (like \"/search\") into full URLs\n",
    "full_links = [\n",
    "    link if link.startswith(\"http\") else base_url + link\n",
    "    for link in valid_links\n",
    "]\n",
    "\n",
    "print(\"Valid links found:\", len(full_links))\n",
    "full_links[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf38dd",
   "metadata": {},
   "source": [
    "Filter only PDF links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b029c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files found: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_links = [link for link in links if link.endswith(\".pdf\")]\n",
    "print(\"PDF files found:\", len(pdf_links))\n",
    "pdf_links[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c065c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error visiting: https://www.treasury.gov.lk http://www.portcitycolombo.gov.lk HTTPSConnectionPool(host='www.treasury.gov.lk%20http', port=443): Max retries exceeded with url: /www.portcitycolombo.gov.lk (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000020D21BE6090>: Failed to resolve 'www.treasury.gov.lk%20http' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 19\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(r\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Extract all links on this page\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     links \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\requests\\models.py:931\u001b[0m, in \u001b[0;36mResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# Fallback to auto-detected encoding.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapparent_encoding\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# Decode unicode from given encoding.\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\requests\\models.py:793\u001b[0m, in \u001b[0;36mResponse.apparent_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chardet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chardet\u001b[38;5;241m.\u001b[39mdetect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;66;03m# If no character detection library is available, we'll fall back\u001b[39;00m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;66;03m# to a standard Python utf-8 str.\u001b[39;00m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\chardet\\__init__.py:41\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m         byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(byte_str)\n\u001b[0;32m     40\u001b[0m detector \u001b[38;5;241m=\u001b[39m UniversalDetector()\n\u001b[1;32m---> 41\u001b[0m detector\u001b[38;5;241m.\u001b[39mfeed(byte_str)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detector\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\chardet\\universaldetector.py:211\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers\u001b[38;5;241m.\u001b[39mappend(Latin1Prober())\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prober \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prober\u001b[38;5;241m.\u001b[39mfeed(byte_str) \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mFOUND_IT:\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mcharset_name,\n\u001b[0;32m    213\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mget_confidence(),\n\u001b[0;32m    214\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mlanguage}\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\chardet\\charsetgroupprober.py:71\u001b[0m, in \u001b[0;36mCharSetGroupProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prober\u001b[38;5;241m.\u001b[39mactive:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m state \u001b[38;5;241m=\u001b[39m prober\u001b[38;5;241m.\u001b[39mfeed(byte_str)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\chardet\\sbcharsetprober.py:98\u001b[0m, in \u001b[0;36mSingleByteCharSetProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     96\u001b[0m language_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mlanguage_model\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m byte_str:\n\u001b[1;32m---> 98\u001b[0m     order \u001b[38;5;241m=\u001b[39m char_to_order_map\u001b[38;5;241m.\u001b[39mget(char, CharacterCategory\u001b[38;5;241m.\u001b[39mUNDEFINED)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# XXX: This was SYMBOL_CAT_ORDER before, with a value of 250, but\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m#      CharacterCategory.SYMBOL is actually 253, so we use CONTROL\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m#      to make it closer to the original intent. The only difference\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m#      is whether or not we count digits and control characters for\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m#      _total_char purposes.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m order \u001b[38;5;241m<\u001b[39m CharacterCategory\u001b[38;5;241m.\u001b[39mCONTROL:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "visited = set()\n",
    "pdf_links = []\n",
    "\n",
    "# Start with homepage\n",
    "to_visit = [base_url]\n",
    "\n",
    "while to_visit:\n",
    "    url = to_visit.pop()\n",
    "    if url in visited:\n",
    "        continue\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Extract all links on this page\n",
    "        links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\")]\n",
    "\n",
    "        for link in links:\n",
    "            # Full link\n",
    "            full = link if link.startswith(\"http\") else base_url + link\n",
    "\n",
    "            # If PDF, save it\n",
    "            if full.endswith(\".pdf\"):\n",
    "                pdf_links.append(full)\n",
    "\n",
    "            # If it's still inside treasury.gov.lk, add for crawling\n",
    "            elif base_url in full:\n",
    "                to_visit.append(full)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error visiting:\", url, e)\n",
    "\n",
    "print(\"Total PDF files found:\", len(pdf_links))\n",
    "for pdf in pdf_links[:10]:\n",
    "    print(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a28cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs found: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "base_url = \"https://www.treasury.gov.lk/web/guest/home\"\n",
    "to_visit = [base_url]\n",
    "visited = set()\n",
    "pdf_links = []\n",
    "\n",
    "while to_visit:\n",
    "    url = to_visit.pop()\n",
    "    if url in visited:\n",
    "        continue\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            link = a[\"href\"]\n",
    "            full_url = urljoin(url, link)\n",
    "\n",
    "            if full_url.endswith(\".pdf\"):\n",
    "                pdf_links.append(full_url)\n",
    "            elif base_url in full_url and full_url not in visited:\n",
    "                to_visit.append(full_url)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping:\", url, \"Error:\", e)\n",
    "\n",
    "print(\"Total PDFs found:\", len(pdf_links))\n",
    "print(pdf_links[:10])  # show first 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63f05e",
   "metadata": {},
   "source": [
    "PDF files in budegt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ea7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camelot-py\n",
      "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (8.1.7)\n",
      "Collecting chardet>=5.1.0 (from camelot-py)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (1.26.4)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (3.1.5)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (20250506)\n",
      "Collecting pypdf<6.0,>=4.0 (from camelot-py)\n",
      "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pandas>=2.2.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (2.2.2)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (0.9.0)\n",
      "Collecting opencv-python-headless>=4.7.0.68 (from camelot-py)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pypdfium2>=4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (4.30.0)\n",
      "Requirement already satisfied: pillow>=10.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from camelot-py) (10.4.0)\n",
      "Requirement already satisfied: distro in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer-six>=20240706->camelot-py) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer-six>=20240706->camelot-py) (43.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click>=8.0.1->camelot-py) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (2.21)\n",
      "Collecting numpy>=1.26.1 (from camelot-py)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openpyxl>=3.1.0->camelot-py) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py) (1.16.0)\n",
      "Downloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
      "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
      "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.0 MB 5.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.0 MB 5.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.0 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 5.1 MB/s  0:00:02\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/38.9 MB 3.3 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.6/38.9 MB 3.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.6/38.9 MB 4.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 3.7/38.9 MB 4.3 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 4.7/38.9 MB 4.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.8/38.9 MB 4.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 6.6/38.9 MB 4.4 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 7.3/38.9 MB 4.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 8.1/38.9 MB 4.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 8.9/38.9 MB 4.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 10.0/38.9 MB 4.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 10.7/38.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 11.5/38.9 MB 4.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 12.3/38.9 MB 4.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 13.1/38.9 MB 4.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 14.2/38.9 MB 4.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 15.2/38.9 MB 4.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 16.0/38.9 MB 4.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 17.0/38.9 MB 4.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 18.1/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 18.9/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.7/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 20.4/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 21.5/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 22.3/38.9 MB 4.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 23.1/38.9 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.9/38.9 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.6/38.9 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.4/38.9 MB 4.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 26.5/38.9 MB 4.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/38.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 29.1/38.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 29.9/38.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 30.7/38.9 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 31.7/38.9 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.5/38.9 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.6/38.9 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.4/38.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.9/38.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/38.9 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.9/38.9 MB 4.0 MB/s  0:00:09\n",
      "Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 3.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.4/12.6 MB 3.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.1/12.6 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.6 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.0/12.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.1/12.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 4.2 MB/s  0:00:03\n",
      "Installing collected packages: pypdf, numpy, chardet, opencv-python-headless, tabula-py, camelot-py\n",
      "\n",
      "   ---------------------------------------- 0/6 [pypdf]\n",
      "   ---------------------------------------- 0/6 [pypdf]Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: numpy\n",
      "   ---------------------------------------- 0/6 [pypdf]\n",
      "    Found existing installation: numpy 1.26.4\n",
      "   ---------------------------------------- 0/6 [pypdf]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "  Attempting uninstall: chardet\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "    Found existing installation: chardet 4.0.0\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "    Uninstalling chardet-4.0.0:\n",
      "   ------ --------------------------------- 1/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [chardet]\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "   ------------- -------------------------- 2/6 [chardet]\n",
      "   ------------- -------------------------- 2/6 [chardet]\n",
      "   ------------- -------------------------- 2/6 [chardet]\n",
      "   -------------------- ------------------- 3/6 [opencv-python-headless]\n",
      "   -------------------- ------------------- 3/6 [opencv-python-headless]\n",
      "   -------------------- ------------------- 3/6 [opencv-python-headless]\n",
      "   -------------------- ------------------- 3/6 [opencv-python-headless]\n",
      "   -------------------- ------------------- 3/6 [opencv-python-headless]\n",
      "   -------------------------- ------------- 4/6 [tabula-py]\n",
      "   ---------------------------------------- 6/6 [camelot-py]\n",
      "\n",
      "Successfully installed camelot-py-1.0.9 chardet-5.2.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 pypdf-5.9.0 tabula-py-2.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install camelot-py tabula-py pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b66ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15952\\243857506.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBudget Speech 2025-rp.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tables \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "pdf_path = \"Budget Speech 2025-rp.pdf\"\n",
    "\n",
    "tables = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            df[\"page\"] = i\n",
    "            tables.append(df)\n",
    "\n",
    "print(f\"✅ Extracted {len(tables)} tables from the PDF\")\n",
    "\n",
    "# Preview first table if available\n",
    "if tables:\n",
    "    print(tables[0].head())\n",
    "else:\n",
    "    print(\"⚠️ No tables detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d65443e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_raw/tables\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tables):\n\u001b[0;32m      7\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     table\u001b[38;5;241m.\u001b[39mto_csv(csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tables' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_folder = \"data_raw/tables\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    csv_path = os.path.join(output_folder, f\"table_{i+1}.csv\")\n",
    "    table.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e2efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.5 MB 4.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/15.5 MB 4.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.6/15.5 MB 5.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.5 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 4.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.5/15.5 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.6/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.4/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.7/15.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.8/15.5 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.8/15.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 4.8 MB/s  0:00:03\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc306908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15952\\243857506.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBudget Speech 2025-rp.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tables \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "pdf_path = \"Budget Speech 2025-rp.pdf\"\n",
    "\n",
    "tables = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            df[\"page\"] = i\n",
    "            tables.append(df)\n",
    "\n",
    "print(f\"✅ Extracted {len(tables)} tables from the PDF\")\n",
    "\n",
    "# Preview first table if available\n",
    "if tables:\n",
    "    print(tables[0].head())\n",
    "else:\n",
    "    print(\"⚠️ No tables detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28668477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62f934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 116 tables from the PDF\n",
      "      0     1     2            3     4     5       6     7  page\n",
      "0  Item  2023               2024                2025         159\n",
      "1  None  None  None  Provisional  None  None  Budget  None   159\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "pdf_path = \"Budget Speech 2025-rp.pdf\"\n",
    "\n",
    "tables = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            df[\"page\"] = i\n",
    "            tables.append(df)\n",
    "\n",
    "print(f\"✅ Extracted {len(tables)} tables from the PDF\")\n",
    "\n",
    "# Preview first table if available\n",
    "if tables:\n",
    "    print(tables[0].head())\n",
    "else:\n",
    "    print(\"⚠️ No tables detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e160585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7126bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20ad980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b480531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_raw/tables\\table_1.csv\n",
      "Saved: data_raw/tables\\table_2.csv\n",
      "Saved: data_raw/tables\\table_3.csv\n",
      "Saved: data_raw/tables\\table_4.csv\n",
      "Saved: data_raw/tables\\table_5.csv\n",
      "Saved: data_raw/tables\\table_6.csv\n",
      "Saved: data_raw/tables\\table_7.csv\n",
      "Saved: data_raw/tables\\table_8.csv\n",
      "Saved: data_raw/tables\\table_9.csv\n",
      "Saved: data_raw/tables\\table_10.csv\n",
      "Saved: data_raw/tables\\table_11.csv\n",
      "Saved: data_raw/tables\\table_12.csv\n",
      "Saved: data_raw/tables\\table_13.csv\n",
      "Saved: data_raw/tables\\table_14.csv\n",
      "Saved: data_raw/tables\\table_15.csv\n",
      "Saved: data_raw/tables\\table_16.csv\n",
      "Saved: data_raw/tables\\table_17.csv\n",
      "Saved: data_raw/tables\\table_18.csv\n",
      "Saved: data_raw/tables\\table_19.csv\n",
      "Saved: data_raw/tables\\table_20.csv\n",
      "Saved: data_raw/tables\\table_21.csv\n",
      "Saved: data_raw/tables\\table_22.csv\n",
      "Saved: data_raw/tables\\table_23.csv\n",
      "Saved: data_raw/tables\\table_24.csv\n",
      "Saved: data_raw/tables\\table_25.csv\n",
      "Saved: data_raw/tables\\table_26.csv\n",
      "Saved: data_raw/tables\\table_27.csv\n",
      "Saved: data_raw/tables\\table_28.csv\n",
      "Saved: data_raw/tables\\table_29.csv\n",
      "Saved: data_raw/tables\\table_30.csv\n",
      "Saved: data_raw/tables\\table_31.csv\n",
      "Saved: data_raw/tables\\table_32.csv\n",
      "Saved: data_raw/tables\\table_33.csv\n",
      "Saved: data_raw/tables\\table_34.csv\n",
      "Saved: data_raw/tables\\table_35.csv\n",
      "Saved: data_raw/tables\\table_36.csv\n",
      "Saved: data_raw/tables\\table_37.csv\n",
      "Saved: data_raw/tables\\table_38.csv\n",
      "Saved: data_raw/tables\\table_39.csv\n",
      "Saved: data_raw/tables\\table_40.csv\n",
      "Saved: data_raw/tables\\table_41.csv\n",
      "Saved: data_raw/tables\\table_42.csv\n",
      "Saved: data_raw/tables\\table_43.csv\n",
      "Saved: data_raw/tables\\table_44.csv\n",
      "Saved: data_raw/tables\\table_45.csv\n",
      "Saved: data_raw/tables\\table_46.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_folder = \"data_raw/tables\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    csv_path = os.path.join(output_folder, f\"table_{i+1}.csv\")\n",
    "    table.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa71418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 All tables combined into: data_raw/all_budget_tables.csv\n"
     ]
    }
   ],
   "source": [
    "all_tables = pd.concat(tables, ignore_index=True)\n",
    "all_tables.to_csv(\"data_raw/all_budget_tables.csv\", index=False)\n",
    "print(\"💾 All tables combined into: data_raw/all_budget_tables.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1eeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_tables.dropna(how=\"all\")  # remove empty rows\n",
    "df.columns = [str(c).strip() for c in df.columns]  # clean col names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca09c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Cleaned CSV saved as: data_raw/cleaned_budget_tables.csv\n",
      "      0                                                1     2            3  \\\n",
      "0  None                                             None  None  Provisional   \n",
      "1                                                   Item                      \n",
      "2        Total Receipts other than Government Borrowings                      \n",
      "3                              Total Primary Expenditure                      \n",
      "4                                              Recurrent                      \n",
      "\n",
      "             4     5       6     7  page    8  ...   12   13   14   15   16  \\\n",
      "0         None  None  Budget  None   159  NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
      "1  Rs. Billion           NaN   NaN   160  NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
      "2        5,042           NaN   NaN   160  NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
      "3        4,285           NaN   NaN   160  NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
      "4        2,970           NaN   NaN   160  NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    17   18   19   20   21  \n",
      "0  NaN  NaN  NaN  NaN  NaN  \n",
      "1  NaN  NaN  NaN  NaN  NaN  \n",
      "2  NaN  NaN  NaN  NaN  NaN  \n",
      "3  NaN  NaN  NaN  NaN  NaN  \n",
      "4  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove fully empty rows\n",
    "df = all_tables.dropna(how=\"all\")\n",
    "\n",
    "# Remove fully empty columns\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# Clean column names (strip spaces, unify names)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "# Remove duplicate header rows (where \"Item\" or \"2023\" appears again mid-table)\n",
    "df = df[df.columns].loc[df[df.columns[0]] != \"Item\"]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(\"data_raw/cleaned_budget_tables.csv\", index=False)\n",
    "\n",
    "print(\"✨ Cleaned CSV saved as: data_raw/cleaned_budget_tables.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb0504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 46 clean tables\n",
      "💾 Saved as data_raw/filtered_budget_tables.csv\n",
      "  0                                                  1 2 3            4 5  \\\n",
      "0                                                 Item      Rs. Billion     \n",
      "1      Total Receipts other than Government Borrowings            5,042     \n",
      "2                            Total Primary Expenditure            4,285     \n",
      "3                                            Recurrent            2,970     \n",
      "4                                              Capital            1,315     \n",
      "5                                Debt Service Payments            4,550     \n",
      "6                                    Interest Payments            2,950     \n",
      "7                                      Debt Repayments            1,600     \n",
      "8                      Provision for Advanced Accounts                7     \n",
      "9    Adjustments for book/cash Value of Government ...              200     \n",
      "\n",
      "   page    6    7    8  ...   12   13   14   15   16   17   18   19   20   21  \n",
      "0   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "1   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "2   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "3   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "4   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "5   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "6   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "7   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "8   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9   160  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[10 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "pdf_path = \"Budget Speech 2025-rp.pdf\"\n",
    "\n",
    "tables = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            \n",
    "            # Skip empty or junk tables (less than 3 columns or 3 rows)\n",
    "            if df.shape[1] < 3 or df.shape[0] < 3:\n",
    "                continue\n",
    "            \n",
    "            # Keep page info\n",
    "            df[\"page\"] = i\n",
    "            tables.append(df)\n",
    "\n",
    "print(f\"✅ Extracted {len(tables)} clean tables\")\n",
    "\n",
    "# Concatenate only valid tables\n",
    "if tables:\n",
    "    all_tables = pd.concat(tables, ignore_index=True)\n",
    "    \n",
    "    # Drop rows/columns that are all NaN\n",
    "    all_tables = all_tables.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    \n",
    "    # Save cleaned file\n",
    "    all_tables.to_csv(\"data_raw/filtered_budget_tables.csv\", index=False)\n",
    "    print(\"💾 Saved as data_raw/filtered_budget_tables.csv\")\n",
    "    \n",
    "    # Preview\n",
    "    print(all_tables.head(10))\n",
    "else:\n",
    "    print(\"⚠️ No valid tables detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b977d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: data_raw/tables\\table_1.csv\n",
      "✅ Saved: data_raw/tables\\table_2.csv\n",
      "✅ Saved: data_raw/tables\\table_3.csv\n",
      "✅ Saved: data_raw/tables\\table_4.csv\n",
      "✅ Saved: data_raw/tables\\table_5.csv\n",
      "✅ Saved: data_raw/tables\\table_6.csv\n",
      "✅ Saved: data_raw/tables\\table_7.csv\n",
      "✅ Saved: data_raw/tables\\table_8.csv\n",
      "✅ Saved: data_raw/tables\\table_9.csv\n",
      "✅ Saved: data_raw/tables\\table_10.csv\n",
      "✅ Saved: data_raw/tables\\table_11.csv\n",
      "✅ Saved: data_raw/tables\\table_12.csv\n",
      "✅ Saved: data_raw/tables\\table_13.csv\n",
      "✅ Saved: data_raw/tables\\table_14.csv\n",
      "✅ Saved: data_raw/tables\\table_15.csv\n",
      "✅ Saved: data_raw/tables\\table_16.csv\n",
      "✅ Saved: data_raw/tables\\table_17.csv\n",
      "✅ Saved: data_raw/tables\\table_18.csv\n",
      "✅ Saved: data_raw/tables\\table_19.csv\n",
      "✅ Saved: data_raw/tables\\table_20.csv\n",
      "✅ Saved: data_raw/tables\\table_21.csv\n",
      "✅ Saved: data_raw/tables\\table_22.csv\n",
      "✅ Saved: data_raw/tables\\table_23.csv\n",
      "✅ Saved: data_raw/tables\\table_24.csv\n",
      "✅ Saved: data_raw/tables\\table_25.csv\n",
      "✅ Saved: data_raw/tables\\table_26.csv\n",
      "✅ Saved: data_raw/tables\\table_27.csv\n",
      "✅ Saved: data_raw/tables\\table_28.csv\n",
      "✅ Saved: data_raw/tables\\table_29.csv\n",
      "✅ Saved: data_raw/tables\\table_30.csv\n",
      "✅ Saved: data_raw/tables\\table_31.csv\n",
      "✅ Saved: data_raw/tables\\table_32.csv\n",
      "✅ Saved: data_raw/tables\\table_33.csv\n",
      "✅ Saved: data_raw/tables\\table_34.csv\n",
      "✅ Saved: data_raw/tables\\table_35.csv\n",
      "✅ Saved: data_raw/tables\\table_36.csv\n",
      "✅ Saved: data_raw/tables\\table_37.csv\n",
      "✅ Saved: data_raw/tables\\table_38.csv\n",
      "✅ Saved: data_raw/tables\\table_39.csv\n",
      "✅ Saved: data_raw/tables\\table_40.csv\n",
      "✅ Saved: data_raw/tables\\table_41.csv\n",
      "✅ Saved: data_raw/tables\\table_42.csv\n",
      "✅ Saved: data_raw/tables\\table_43.csv\n",
      "✅ Saved: data_raw/tables\\table_44.csv\n",
      "✅ Saved: data_raw/tables\\table_45.csv\n",
      "✅ Saved: data_raw/tables\\table_46.csv\n",
      "✅ Saved: data_raw/tables\\table_47.csv\n",
      "✅ Saved: data_raw/tables\\table_48.csv\n",
      "✅ Saved: data_raw/tables\\table_49.csv\n",
      "✅ Saved: data_raw/tables\\table_50.csv\n",
      "✅ Saved: data_raw/tables\\table_51.csv\n",
      "✅ Saved: data_raw/tables\\table_52.csv\n",
      "✅ Saved: data_raw/tables\\table_53.csv\n",
      "✅ Saved: data_raw/tables\\table_54.csv\n",
      "✅ Saved: data_raw/tables\\table_55.csv\n",
      "✅ Saved: data_raw/tables\\table_56.csv\n",
      "✅ Saved: data_raw/tables\\table_57.csv\n",
      "✅ Saved: data_raw/tables\\table_58.csv\n",
      "✅ Saved: data_raw/tables\\table_59.csv\n",
      "✅ Saved: data_raw/tables\\table_60.csv\n",
      "✅ Saved: data_raw/tables\\table_61.csv\n",
      "✅ Saved: data_raw/tables\\table_62.csv\n",
      "✅ Saved: data_raw/tables\\table_63.csv\n",
      "✅ Saved: data_raw/tables\\table_64.csv\n",
      "✅ Saved: data_raw/tables\\table_65.csv\n",
      "✅ Saved: data_raw/tables\\table_66.csv\n",
      "✅ Saved: data_raw/tables\\table_67.csv\n",
      "✅ Saved: data_raw/tables\\table_68.csv\n",
      "✅ Saved: data_raw/tables\\table_69.csv\n",
      "✅ Saved: data_raw/tables\\table_70.csv\n",
      "✅ Saved: data_raw/tables\\table_71.csv\n",
      "✅ Saved: data_raw/tables\\table_72.csv\n",
      "✅ Saved: data_raw/tables\\table_73.csv\n",
      "✅ Saved: data_raw/tables\\table_74.csv\n",
      "✅ Saved: data_raw/tables\\table_75.csv\n",
      "✅ Saved: data_raw/tables\\table_76.csv\n",
      "✅ Saved: data_raw/tables\\table_77.csv\n",
      "✅ Saved: data_raw/tables\\table_78.csv\n",
      "✅ Saved: data_raw/tables\\table_79.csv\n",
      "✅ Saved: data_raw/tables\\table_80.csv\n",
      "✅ Saved: data_raw/tables\\table_81.csv\n",
      "✅ Saved: data_raw/tables\\table_82.csv\n",
      "✅ Saved: data_raw/tables\\table_83.csv\n",
      "✅ Saved: data_raw/tables\\table_84.csv\n",
      "✅ Saved: data_raw/tables\\table_85.csv\n",
      "✅ Saved: data_raw/tables\\table_86.csv\n",
      "✅ Saved: data_raw/tables\\table_87.csv\n",
      "✅ Saved: data_raw/tables\\table_88.csv\n",
      "✅ Saved: data_raw/tables\\table_89.csv\n",
      "✅ Saved: data_raw/tables\\table_90.csv\n",
      "✅ Saved: data_raw/tables\\table_91.csv\n",
      "✅ Saved: data_raw/tables\\table_92.csv\n",
      "✅ Saved: data_raw/tables\\table_93.csv\n",
      "✅ Saved: data_raw/tables\\table_94.csv\n",
      "✅ Saved: data_raw/tables\\table_95.csv\n",
      "✅ Saved: data_raw/tables\\table_96.csv\n",
      "✅ Saved: data_raw/tables\\table_97.csv\n",
      "✅ Saved: data_raw/tables\\table_98.csv\n",
      "✅ Saved: data_raw/tables\\table_99.csv\n",
      "✅ Saved: data_raw/tables\\table_100.csv\n",
      "✅ Saved: data_raw/tables\\table_101.csv\n",
      "✅ Saved: data_raw/tables\\table_102.csv\n",
      "✅ Saved: data_raw/tables\\table_103.csv\n",
      "✅ Saved: data_raw/tables\\table_104.csv\n",
      "✅ Saved: data_raw/tables\\table_105.csv\n",
      "✅ Saved: data_raw/tables\\table_106.csv\n",
      "✅ Saved: data_raw/tables\\table_107.csv\n",
      "✅ Saved: data_raw/tables\\table_108.csv\n",
      "✅ Saved: data_raw/tables\\table_109.csv\n",
      "✅ Saved: data_raw/tables\\table_110.csv\n",
      "✅ Saved: data_raw/tables\\table_111.csv\n",
      "✅ Saved: data_raw/tables\\table_112.csv\n",
      "✅ Saved: data_raw/tables\\table_113.csv\n",
      "✅ Saved: data_raw/tables\\table_114.csv\n",
      "✅ Saved: data_raw/tables\\table_115.csv\n",
      "✅ Saved: data_raw/tables\\table_116.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Folder to save individual tables\n",
    "output_folder = \"data_raw/tables\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save each table separately\n",
    "for i, table in enumerate(tables, start=1):  # start=1 for table_1, table_2...\n",
    "    try:\n",
    "        # Convert to DataFrame if not already\n",
    "        df = pd.DataFrame(table)\n",
    "        \n",
    "        # File path\n",
    "        csv_path = os.path.join(output_folder, f\"table_{i}.csv\")\n",
    "        \n",
    "        # Save\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not save table {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ccfc4",
   "metadata": {},
   "source": [
    "Tute 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d29906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File loaded successfully!\n",
      "📊 Shape of data: (2, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provisional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Budget</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1   2            3   4   5       6   7  page\n",
       "0  Item  2023.0 NaN         2024 NaN NaN    2025 NaN   159\n",
       "1   NaN     NaN NaN  Provisional NaN NaN  Budget NaN   159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INFO ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       1 non-null      object \n",
      " 1   1       1 non-null      float64\n",
      " 2   2       0 non-null      float64\n",
      " 3   3       2 non-null      object \n",
      " 4   4       0 non-null      float64\n",
      " 5   5       0 non-null      float64\n",
      " 6   6       2 non-null      object \n",
      " 7   7       0 non-null      float64\n",
      " 8   page    2 non-null      int64  \n",
      "dtypes: float64(5), int64(1), object(3)\n",
      "memory usage: 276.0+ bytes\n",
      "None\n",
      "\n",
      "--- DESCRIPTIVE STATS ---\n",
      "           0       1    2     3    4    5     6    7   page\n",
      "count      1     1.0  0.0     2  0.0  0.0     2  0.0    2.0\n",
      "unique     1     NaN  NaN     2  NaN  NaN     2  NaN    NaN\n",
      "top     Item     NaN  NaN  2024  NaN  NaN  2025  NaN    NaN\n",
      "freq       1     NaN  NaN     1  NaN  NaN     1  NaN    NaN\n",
      "mean     NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "std      NaN     NaN  NaN   NaN  NaN  NaN   NaN  NaN    0.0\n",
      "min      NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "25%      NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "50%      NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "75%      NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "max      NaN  2023.0  NaN   NaN  NaN  NaN   NaN  NaN  159.0\n",
      "\n",
      "✅ Cleaned column names: ['0', '1', '2', '3', '4', '5', '6', '7', 'page']\n",
      "\n",
      "🔍 Missing values per column:\n",
      "0       1\n",
      "1       1\n",
      "2       2\n",
      "3       0\n",
      "4       2\n",
      "5       2\n",
      "6       0\n",
      "7       2\n",
      "page    0\n",
      "dtype: int64\n",
      "\n",
      "🗑️ Removed 0 duplicate rows\n",
      "\n",
      "💾 Cleaned dataset saved to: data_clean/table_1_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\1949642375.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\1949642375.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Provisional</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Budget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1    2            3    4    5       6    7  page\n",
       "0     Item  2023.0  0.0         2024  0.0  0.0    2025  0.0   159\n",
       "1  Unknown     0.0  0.0  Provisional  0.0  0.0  Budget  0.0   159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Week 2: Pandas Practice ===\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. LOAD RAW DATA\n",
    "raw_file = \"data_raw/tables/table_1.csv\"   # pick any saved table\n",
    "df = pd.read_csv(raw_file)\n",
    "\n",
    "print(\"✅ File loaded successfully!\")\n",
    "print(\"📊 Shape of data:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# 2. INSPECT DATA\n",
    "print(\"\\n--- INFO ---\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n--- DESCRIPTIVE STATS ---\")\n",
    "print(df.describe(include=\"all\"))\n",
    "\n",
    "# 3. CLEAN COLUMN NAMES\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "print(\"\\n✅ Cleaned column names:\", df.columns.tolist())\n",
    "\n",
    "# 4. HANDLE MISSING VALUES\n",
    "print(\"\\n🔍 Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Example cleaning: fill missing numbers with 0, text with \"Unknown\"\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [\"float64\", \"int64\"]:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# 5. REMOVE DUPLICATES\n",
    "before = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "after = df.shape[0]\n",
    "print(f\"\\n🗑️ Removed {before - after} duplicate rows\")\n",
    "\n",
    "# 6. SAVE CLEANED FILE\n",
    "output_file = \"data_clean/table_1_clean.csv\"\n",
    "os.makedirs(\"data_clean\", exist_ok=True)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n💾 Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "# 7. PREVIEW CLEANED DATA\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d85439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Provisional</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Budget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1    2            3    4    5       6    7  page\n",
       "0     Item  2023.0  0.0         2024  0.0  0.0    2025  0.0   159\n",
       "1  Unknown     0.0  0.0  Provisional  0.0  0.0  Budget  0.0   159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_check = pd.read_csv(\"data_clean/table_1_clean.csv\")\n",
    "display(df_check.head(10))   # show first 10 rows nicely formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8ac351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Provisional</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Budget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1    2            3    4    5       6    7  page\n",
       "0     Item  2023.0  0.0         2024  0.0  0.0    2025  0.0   159\n",
       "1  Unknown     0.0  0.0  Provisional  0.0  0.0  Budget  0.0   159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_check = pd.read_csv(\"data_clean/table_1_clean.csv\")\n",
    "display(df_check.head(10))   # show first 10 rows nicely formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96677166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\2943246927.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables found: 4\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.26LKR  304.81LKR\n",
      "1      GBP  401.70LKR  414.38LKR\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "tables = pd.read_html(response.text)\n",
    "\n",
    "# Check how many tables were found\n",
    "print(f\"Total tables found: {len(tables)}\")\n",
    "\n",
    "# Print the first table\n",
    "print(tables[0].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718234b4",
   "metadata": {},
   "source": [
    "Tute 2 gemmini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1faa8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "\n",
      "First 500 characters of the HTML content:\n",
      "<!DOCTYPE html><html lang=\"en\"><head><script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-X5XT78QC7P\"></script><script>\n",
      "            window.dataLayer = window.dataLayer || [];\n",
      "            function gtag(){dataLayer.push(arguments);}\n",
      "            gtag('js', new Date());\n",
      "            gtag('config', 'G-X5XT78QC7P', {\n",
      "              page_path: window.location.pathname,\n",
      "            });\n",
      "          </script><meta name=\"viewport\" content=\"width=device-width\"/><meta charSet=\"utf-8\"/><link href=\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL for the Ministry of Finance website\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the HTTP status code to check if the request was successful\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "# Check if the request was successful before printing the HTML\n",
    "if response.status_code == 200:\n",
    "    print(\"\\nFirst 500 characters of the HTML content:\")\n",
    "    print(response.text[:500])\n",
    "else:\n",
    "    print(\"\\nFailed to retrieve the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c771f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 tables on the page.\n",
      "\n",
      "First 5 rows of the first table:\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.26LKR  304.81LKR\n",
      "1      GBP  401.70LKR  414.38LKR\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://www.treasury.gov.lk\" # Replace with a URL that has a table\n",
    "\n",
    "try:\n",
    "    # Use pandas to find and read all tables on the page\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # Print the number of tables found\n",
    "    print(f\"Found {len(tables)} tables on the page.\")\n",
    "    \n",
    "    # The first table is usually at index 0\n",
    "    if tables:\n",
    "        df_table = tables[0]\n",
    "        print(\"\\nFirst 5 rows of the first table:\")\n",
    "        print(df_table.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading HTML tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d271dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read CSV from URL: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is a hypothetical URL to a CSV file on the website\n",
    "csv_url = \"https://www.treasury.gov.lk/data/some-report.csv\"\n",
    "\n",
    "try:\n",
    "    df_csv = pd.read_csv(csv_url)\n",
    "    print(\"Successfully read CSV data:\")\n",
    "    print(df_csv.head())\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read CSV from URL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec66f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from the API: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/api/v1/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# This is a hypothetical API endpoint on the website\n",
    "json_url = \"https://www.treasury.gov.lk/api/v1/data\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(json_url)\n",
    "    response.raise_for_status() # This will raise an error for bad responses\n",
    "    data = response.json()\n",
    "    \n",
    "    # Convert the JSON data to a pandas DataFrame\n",
    "    df_json = pd.DataFrame(data)\n",
    "    print(\"Successfully read JSON data:\")\n",
    "    print(df_json.head())\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to retrieve data from the API: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the JSON data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28117e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading CSV: HTTP Error 404: Not Found\n",
      "Error reading JSON: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# This is a hypothetical URL to a page containing a link to a CSV\n",
    "# Let's say we find the raw link by inspecting the page\n",
    "raw_csv_url = \"https://www.treasury.gov.lk/content/documents/2025/budget_report.csv\"\n",
    "\n",
    "try:\n",
    "    df_csv = pd.read_csv(raw_csv_url)\n",
    "    print(\"Successfully read CSV data from the raw URL:\")\n",
    "    print(df_csv.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "\n",
    "# This applies to JSON too\n",
    "raw_json_url = \"https://www.treasury.gov.lk/api/v2/financial-data.json\"\n",
    "\n",
    "try:\n",
    "    df_json = pd.read_json(raw_json_url)\n",
    "    print(\"\\nSuccessfully read JSON data from the raw URL:\")\n",
    "    print(df_json.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7764ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from the API: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/api/v1/projects\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# This is a hypothetical API URL that returns nested JSON\n",
    "# (e.g., a list of budget projects with nested details)\n",
    "api_url = \"https://www.treasury.gov.lk/api/v1/projects\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # The JSON data is likely a list of objects under a key, e.g., 'results'\n",
    "    # We extract that list first.\n",
    "    # We can inspect the data with `print(data)` to find the correct key.\n",
    "    if 'projects' in data:\n",
    "        results = data['projects']\n",
    "    else:\n",
    "        results = data # If the data is already a list\n",
    "\n",
    "    # Use json_normalize to flatten the nested data into a DataFrame\n",
    "    df_normalized = pd.json_normalize(results)\n",
    "    \n",
    "    print(\"Successfully read and flattened JSON data:\")\n",
    "    print(df_normalized.head())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to retrieve data from the API: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the JSON data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c57ac4",
   "metadata": {},
   "source": [
    "BeautifulSoup-Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dcd01",
   "metadata": {},
   "source": [
    "Step 1: Set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40419c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f03219",
   "metadata": {},
   "source": [
    "Step 2: Define your URL and ethical headers.\n",
    "As discussed in your lectures, using a proper User-Agent is an ethical best practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f6a3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489bd36",
   "metadata": {},
   "source": [
    "Step 3: Fetch the HTML content.\n",
    "This is where the \n",
    "\n",
    "requests library comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aeb0f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the page.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # This will raise an HTTPError for bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"Successfully fetched the page.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # You can choose to exit the program here if the request fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9a750",
   "metadata": {},
   "source": [
    "3. Extract and Store the Data\n",
    "Now, let's write code to extract the data we identified. You'll need to use your browser's \"Inspect\" tool (F12) to find the correct HTML tags, classes, or IDs for these items.\n",
    "\n",
    "A) Scraping News Headlines:\n",
    "\n",
    "Assume that news headlines are within div tags with a class of \"news-item\" and a sub-element for the title and a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff4e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find any news items with the specified class.\n"
     ]
    }
   ],
   "source": [
    "news_list = soup.find_all('div', class_='news-item')\n",
    "scraped_data = []\n",
    "\n",
    "if news_list:\n",
    "    for item in news_list:\n",
    "        title = item.find('h3', class_='news-title').get_text(strip=True)\n",
    "        link_tag = item.find('a', class_='news-link')\n",
    "        link = url + link_tag['href'] if link_tag and 'href' in link_tag.attrs else 'No Link Found'\n",
    "        scraped_data.append({'Title': title, 'Link': link})\n",
    "\n",
    "    df_news = pd.DataFrame(scraped_data)\n",
    "    print(\"\\nScraped News Headlines:\")\n",
    "    print(df_news.head())\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    df_news.to_csv('news_headlines.csv', index=False)\n",
    "else:\n",
    "    print(\"Could not find any news items with the specified class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a3e3a",
   "metadata": {},
   "source": [
    "B) Scraping HTML Tables:\n",
    "\n",
    "If you find a table on the site, the \n",
    "\n",
    "pandas.read_html() function from your Week 2 lecture is the fastest way to get the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9408c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scraped an HTML table. Found 4 tables.\n",
      "First 5 rows of the table:\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.26LKR  304.81LKR\n",
      "1      GBP  401.70LKR  414.38LKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\981104922.py:3: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This will search the entire page for any HTML tables\n",
    "    tables = pd.read_html(response.text)\n",
    "    \n",
    "    if len(tables) > 0:\n",
    "        df_table = tables[0] # Assume the first table is the one you want\n",
    "        print(f\"\\nSuccessfully scraped an HTML table. Found {len(tables)} tables.\")\n",
    "        print(\"First 5 rows of the table:\")\n",
    "        print(df_table.head())\n",
    "        \n",
    "        # Save the table to a CSV file\n",
    "        df_table.to_csv('financial_table.csv', index=False)\n",
    "    else:\n",
    "        print(\"\\nNo HTML tables were found on the page.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while reading HTML tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a973217",
   "metadata": {},
   "source": [
    "1. Scraping the Latest News and Press Releases\n",
    "This code will navigate to the news section and extract the titles and download links for recent press releases. This is a common and practical use case for web scraping.\n",
    "\n",
    "URL: https://www.treasury.gov.lk/web/press-releases/section/2024.\n",
    "\n",
    "Data Points: For each press release, we want to extract its Date, Title, and the Download link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05e7a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the Press Releases page.\n",
      "\n",
      "Scraped Press Release Data:\n",
      "         Date                                              Title  Download  \\\n",
      "0  18-12-2024  Establishment of the Public Debt Management Of...  Download   \n",
      "1  18-12-2024  Removal of Temporary Suspension on Importation...  Download   \n",
      "2  18-12-2024  Sri Lanka Announces Final Results of the Invit...  Download   \n",
      "3  13-12-2024  SRI LANKA ANNOUNCES THE SUCCESSFUL EXPIRATION ...  Download   \n",
      "4  05-12-2024                       Supplementary Estimates-2024  Download   \n",
      "\n",
      "                                       Download Link  \n",
      "0  https://www.treasury.gov.lk/api/file/1f4a438a-...  \n",
      "1  https://www.treasury.gov.lk/api/file/413ccb47-...  \n",
      "2  https://www.treasury.gov.lk/api/file/8717eef6-...  \n",
      "3  https://www.treasury.gov.lk/api/file/a96d4a52-...  \n",
      "4  https://www.treasury.gov.lk/api/file/e5013dd3-...  \n",
      "\n",
      "Data saved to 'press_releases_data.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\985024911.py:27: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_releases = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the URL for the Press Releases section\n",
    "press_releases_url = \"https://www.treasury.gov.lk/web/press-releases/section/2024\"\n",
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(press_releases_url, headers=headers)\n",
    "    response.raise_for_status() # This will catch bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"Successfully fetched the Press Releases page.\")\n",
    "    \n",
    "    # Let's find the table that contains the press releases.\n",
    "    # We use a CSS selector to target the specific table\n",
    "    # The 'class' attribute is often a good way to identify elements.\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if table:\n",
    "        # Use pandas to read the HTML table directly into a DataFrame\n",
    "        df_releases = pd.read_html(str(table))[0]\n",
    "        \n",
    "        # Now, you'll need to get the download links. They are usually in <a> tags.\n",
    "        # This part requires inspecting the specific HTML structure.\n",
    "        # We can find all links in the table and create a list.\n",
    "        links = [base_url + a['href'] for a in table.find_all('a', href=True)]\n",
    "        \n",
    "        # Add a new column with the download links\n",
    "        df_releases['Download Link'] = links[:len(df_releases)]\n",
    "        \n",
    "        print(\"\\nScraped Press Release Data:\")\n",
    "        print(df_releases.head())\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_releases.to_csv('press_releases_data.csv', index=False)\n",
    "        print(\"\\nData saved to 'press_releases_data.csv'.\")\n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dd453",
   "metadata": {},
   "source": [
    "2. Scraping Publication Links\n",
    "The Ministry of Finance website also has a section for publications. We can scrape this page to get a list of all official reports and their links.\n",
    "\n",
    "URL: https://www.treasury.gov.lk/web/publications.\n",
    "\n",
    "Data Points: We want the titles of the publications and their corresponding links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5d47b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the Publications page.\n",
      "Could not find publications container on the page. Check HTML structure.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the URL for the Publications section\n",
    "publications_url = \"https://www.treasury.gov.lk/web/publications\"\n",
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(publications_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"Successfully fetched the Publications page.\")\n",
    "    \n",
    "    # Find all links to publications. This requires inspecting the page.\n",
    "    # Publications are often in a list or a series of divs.\n",
    "    # Let's assume they are within a parent div with a class 'publications-list'.\n",
    "    \n",
    "    publications_container = soup.find('div', class_='publications-list')\n",
    "    scraped_publications = []\n",
    "    \n",
    "    if publications_container:\n",
    "        # Find all the links within that container\n",
    "        publication_links = publications_container.find_all('a', href=True)\n",
    "        \n",
    "        for link in publication_links:\n",
    "            title = link.get_text(strip=True)\n",
    "            link_href = link['href']\n",
    "            \n",
    "            # The link might be relative, so we join it with the base URL\n",
    "            full_link = requests.compat.urljoin(base_url, link_href)\n",
    "            \n",
    "            scraped_publications.append({'Publication Title': title, 'Link': full_link})\n",
    "\n",
    "        df_publications = pd.DataFrame(scraped_publications)\n",
    "        print(\"\\nScraped Publications Data:\")\n",
    "        print(df_publications.head())\n",
    "        \n",
    "        df_publications.to_csv('publications_links.csv', index=False)\n",
    "        print(\"\\nData saved to 'publications_links.csv'.\")\n",
    "    else:\n",
    "        print(\"Could not find publications container on the page. Check HTML structure.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7ef91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/web//section/2024\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url = \"https://www.treasury.gov.lk/web/exchangerates/section/2024\" # Example URL\n",
    "headers = {'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "    \n",
    "    if len(tables) > 0:\n",
    "        print(f\"Successfully scraped an HTML page and found {len(tables)} tables.\")\n",
    "        \n",
    "        # Access and save the first table (e.g., Currency Exchange Rates)\n",
    "        df_currency = tables[0]\n",
    "        df_currency.to_csv('currency_exchange_rates.csv', index=False)\n",
    "        print(\"First table (Currency Exchange Rates) saved to CSV.\")\n",
    "        \n",
    "        # Access and save the second table\n",
    "        df_table_2 = tables[1]\n",
    "        df_table_2.to_csv('table_2.csv', index=False)\n",
    "        print(\"Second table saved to CSV.\")\n",
    "        \n",
    "        # Access and save the third table\n",
    "        df_table_3 = tables[2]\n",
    "        df_table_3.to_csv('table_3.csv', index=False)\n",
    "        print(\"Third table saved to CSV.\")\n",
    "        \n",
    "        # Access and save the fourth table\n",
    "        df_table_4 = tables[3]\n",
    "        df_table_4.to_csv('table_4.csv', index=False)\n",
    "        print(\"Fourth table saved to CSV.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d44c5854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/web//section/2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://www.treasury.gov.lk/web/exchangerates/section/2024\" # This URL may change. Please verify.\n",
    "headers = {'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "\n",
    "    if len(tables) > 0:\n",
    "        print(f\"Successfully scraped an HTML page and found {len(tables)} tables.\")\n",
    "        \n",
    "        # Use a for loop to iterate through each table and save it\n",
    "        for i, table in enumerate(tables):\n",
    "            table_name = f'table_{i+1}.csv'\n",
    "            table.to_csv(table_name, index=False)\n",
    "            print(f\"Table {i+1} saved to '{table_name}'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7840d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the homepage and found 4 tables.\n",
      "Table 1 saved to 'table_1_economic_indicators.csv'.\n",
      "Table 2 saved to 'table_2_economic_indicators.csv'.\n",
      "Table 3 saved to 'table_3_economic_indicators.csv'.\n",
      "Table 4 saved to 'table_4_economic_indicators.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24068\\741208.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "\n",
    "    if len(tables) > 0:\n",
    "        print(f\"Successfully scraped the homepage and found {len(tables)} tables.\")\n",
    "        \n",
    "        # Save each table to a uniquely named CSV file\n",
    "        for i, table in enumerate(tables):\n",
    "            filename = f\"table_{i+1}_economic_indicators.csv\"\n",
    "            table.to_csv(filename, index=False)\n",
    "            print(f\"Table {i+1} saved to '{filename}'.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No tables found on the homepage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13b0f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a3da13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the scraping process by finding all department links...\n",
      "\n",
      "No data was scraped.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://www.treasury.gov.lk/web",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Step 1: Fetch the main Departments page\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(departments_page_url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 23\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     24\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully fetched the main departments page. Now locating links.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/web"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Define the base URL and the headers for ethical scraping\n",
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "# **IMPORTANT**: Replace this with the actual URL that lists all departments\n",
    "departments_page_url = f\"{base_url}/web/ministry-and-departments\" \n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'\n",
    "}\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "print(\"Starting the scraping process by finding all department links...\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch the main Departments page\n",
    "    response = requests.get(departments_page_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    print(\"Successfully fetched the main departments page. Now locating links.\")\n",
    "\n",
    "    # Step 2: Find all links with the specific department URL pattern.\n",
    "    # The pattern is '/web/department-of-...'\n",
    "    department_links_elements = soup.find_all('a', href=re.compile(r'/web/department-of-'))\n",
    "\n",
    "    unique_links = list(set([requests.compat.urljoin(base_url, link['href']) for link in department_links_elements]))\n",
    "    \n",
    "    print(f\"Found {len(unique_links)} potential department links.\")\n",
    "    \n",
    "    # Step 3: Loop through each unique department URL\n",
    "    for url in unique_links:\n",
    "        print(f\"\\nScraping page: {url}...\")\n",
    "        \n",
    "        # Add a delay to be an ethical scraper\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            department_response = requests.get(url, headers=headers)\n",
    "            department_response.raise_for_status()\n",
    "            department_soup = BeautifulSoup(department_response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the department name from a heading\n",
    "            department_name = department_soup.find('h1').get_text(strip=True) if department_soup.find('h1') else \"Unknown\"\n",
    "\n",
    "            # Step 4: Scrape Vision and Mission statements\n",
    "            vision = \"Not Found\"\n",
    "            mission = \"Not Found\"\n",
    "            \n",
    "            # Find the headings for Vision and Mission (e.g., <h4>)\n",
    "            vision_heading = department_soup.find(['h3', 'h4'], string=re.compile(r'vision', re.I))\n",
    "            mission_heading = department_soup.find(['h3', 'h4'], string=re.compile(r'mission', re.I))\n",
    "\n",
    "            # Find the text that immediately follows the headings\n",
    "            if vision_heading and vision_heading.find_next_sibling(['p', 'div']):\n",
    "                vision = vision_heading.find_next_sibling(['p', 'div']).get_text(strip=True)\n",
    "            \n",
    "            if mission_heading and mission_heading.find_next_sibling(['p', 'div']):\n",
    "                mission = mission_heading.find_next_sibling(['p', 'div']).get_text(strip=True)\n",
    "            \n",
    "            scraped_data.append({\n",
    "                \"Department\": department_name,\n",
    "                \"Vision\": vision,\n",
    "                \"Mission\": mission,\n",
    "                \"URL\": url\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully scraped '{department_name}'. Vision: {vision != 'Not Found'}, Mission: {mission != 'Not Found'}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while parsing {url}: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Step 5: Consolidate data into a single table\n",
    "    if scraped_data:\n",
    "        df = pd.DataFrame(scraped_data)\n",
    "        df.to_excel('all_department_statements.xlsx', index=False)\n",
    "        print(\"\\nData for all departments successfully saved to 'all_department_statements.xlsx'.\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"\\nNo data was scraped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0352a0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34831676",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6f94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fd587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154f35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
