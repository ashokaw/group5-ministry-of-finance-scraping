{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8681d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.2\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (5.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.34.2)\n",
      "Requirement already satisfied: scrapy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (2.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 5)) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (43.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (24.2.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: tldextract in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (5.1.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scrapy->-r requirements.txt (line 6)) (2.0.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\dell\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy->-r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy->-r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tldextract->scrapy->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tldextract->scrapy->-r requirements.txt (line 6)) (3.13.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcee4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "\n",
    "print(\"✅ All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d56da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83488009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and headers\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (https://github.com/your_repo_name)'\n",
    "}\n",
    "\n",
    "# Add a User-Agent to your requests to mimic a browser.\n",
    "# This is a key step in ethical scraping, as covered in Week 7[cite: 1158]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10248b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the page.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Check for bad status codes (4xx or 5xx) [cite: 518, 532]\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"Successfully fetched the page.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2ef37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming news headlines are inside <h2> tags with a specific class\n",
    "news_items = soup.find_all('h2', class_='news-title')\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each item to extract the text and a link\n",
    "for item in news_items:\n",
    "    title = item.get_text(strip=True)\n",
    "    link_tag = item.find('a')\n",
    "    if link_tag:\n",
    "        link = link_tag['href']\n",
    "        scraped_data.append({'Title': title, 'Link': url + link})\n",
    "\n",
    "# Convert to a DataFrame and display\n",
    "df = pd.DataFrame(scraped_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200cd084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 tables on the page.\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.08LKR  304.61LKR\n",
      "1      GBP  397.64LKR  410.70LKR\n"
     ]
    }
   ],
   "source": [
    "# Try to scrape any HTML tables from the page\n",
    "try:\n",
    "    tables = pd.read_html(url)\n",
    "    print(f\"Found {len(tables)} tables on the page.\")\n",
    "    # The first table is usually at index 0. You can inspect others if needed.\n",
    "    df_table = tables[0]\n",
    "    print(df_table.head())\n",
    "except Exception as e:\n",
    "    print(f\"No tables found or an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585097bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x0x7ff7bbc7e415+77285]\n\tGetHandleVerifier [0x0x7ff7bbc7e470+77376]\n\t(No symbol) [0x0x7ff7bba49a6a]\n\t(No symbol) [0x0x7ff7bbaa0406]\n\t(No symbol) [0x0x7ff7bbaa06bc]\n\t(No symbol) [0x0x7ff7bbaf3ac7]\n\t(No symbol) [0x0x7ff7bbac864f]\n\t(No symbol) [0x0x7ff7bbaf087f]\n\t(No symbol) [0x0x7ff7bbac83e3]\n\t(No symbol) [0x0x7ff7bba91521]\n\t(No symbol) [0x0x7ff7bba922b3]\n\tGetHandleVerifier [0x0x7ff7bbf61efd+3107021]\n\tGetHandleVerifier [0x0x7ff7bbf5c29d+3083373]\n\tGetHandleVerifier [0x0x7ff7bbf7bedd+3213485]\n\tGetHandleVerifier [0x0x7ff7bbc9884e+184862]\n\tGetHandleVerifier [0x0x7ff7bbca055f+216879]\n\tGetHandleVerifier [0x0x7ff7bbc87084+113236]\n\tGetHandleVerifier [0x0x7ff7bbc87239+113673]\n\tGetHandleVerifier [0x0x7ff7bbc6e298+11368]\n\tBaseThreadInitThunk [0x0x7ffa8df5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8f69c34c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Wait for up to 10 seconds for the dynamic content (e.g., a table with the ID 'data-table') to be visible [cite: 1053]\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     14\u001b[0m     EC\u001b[38;5;241m.\u001b[39mvisibility_of_element_located((By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-table\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Now that the element has loaded, you can get the page source\u001b[39;00m\n\u001b[0;32m     18\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:138\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x0x7ff7bbc7e415+77285]\n\tGetHandleVerifier [0x0x7ff7bbc7e470+77376]\n\t(No symbol) [0x0x7ff7bba49a6a]\n\t(No symbol) [0x0x7ff7bbaa0406]\n\t(No symbol) [0x0x7ff7bbaa06bc]\n\t(No symbol) [0x0x7ff7bbaf3ac7]\n\t(No symbol) [0x0x7ff7bbac864f]\n\t(No symbol) [0x0x7ff7bbaf087f]\n\t(No symbol) [0x0x7ff7bbac83e3]\n\t(No symbol) [0x0x7ff7bba91521]\n\t(No symbol) [0x0x7ff7bba922b3]\n\tGetHandleVerifier [0x0x7ff7bbf61efd+3107021]\n\tGetHandleVerifier [0x0x7ff7bbf5c29d+3083373]\n\tGetHandleVerifier [0x0x7ff7bbf7bedd+3213485]\n\tGetHandleVerifier [0x0x7ff7bbc9884e+184862]\n\tGetHandleVerifier [0x0x7ff7bbca055f+216879]\n\tGetHandleVerifier [0x0x7ff7bbc87084+113236]\n\tGetHandleVerifier [0x0x7ff7bbc87239+113673]\n\tGetHandleVerifier [0x0x7ff7bbc6e298+11368]\n\tBaseThreadInitThunk [0x0x7ffa8df5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8f69c34c+44]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the driver (make sure the chromedriver is in your path or folder)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for up to 10 seconds for the dynamic content (e.g., a table with the ID 'data-table') to be visible [cite: 1053]\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.ID, 'data-table'))\n",
    "    )\n",
    "\n",
    "    # Now that the element has loaded, you can get the page source\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Scrape the dynamic content from the soup object\n",
    "    dynamic_data_element = soup.find('table', id='data-table')\n",
    "\n",
    "    # Now, you can process the dynamic_data_element with BeautifulSoup\n",
    "    # ... your scraping logic here ...\n",
    "\n",
    "finally:\n",
    "    # Always remember to close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2884bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to data/scraped_treasury_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the scraped data to a CSV file\n",
    "df.to_csv('data/scraped_treasury_data.csv', index=False)\n",
    "print(\"Data saved successfully to data/scraped_treasury_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133460f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.treasury.gov.lk/\"\n",
    "tables = pd.read_html(url)  # returns list of DataFrames\n",
    "df = tables[0]  # first table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318c45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 tables\n",
      "\n",
      "Table 0 shape: (2, 3)\n",
      "  Currency     Buying    Selling\n",
      "0      USD  297.08LKR  304.61LKR\n",
      "1      GBP  397.64LKR  410.70LKR\n",
      "\n",
      "Table 1 shape: (3, 3)\n",
      "                    Unnamed: 0   Year / Month Amount / LKR Bn\n",
      "0  Governement Revenue & Grant  Jan-July 2024          2155.9\n",
      "1       Government Expenditure  Jan-July 2024          3034.4\n",
      "2       Overall Budget Deficit              .               .\n",
      "\n",
      "Table 2 shape: (2, 4)\n",
      "   #               Item   Pettah Dambulla\n",
      "0  1              Samba  0LKR/Kg  0LKR/Kg\n",
      "1  2  Red-Onions(Local)  0LKR/Kg  0LKR/Kg\n",
      "\n",
      "Table 3 shape: (2, 4)\n",
      "     Month/Year  Export  Import  Trade Balance\n",
      "0  Jan-Nov 2023       0       0              0\n",
      "1  Jan-Nov 2022       0       0              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.treasury.gov.lk/\"\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\nTable {i} shape: {table.shape}\")\n",
    "    print(table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862d8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tables[2]  # replace with correct index\n",
    "df.to_csv(\"treasury_data_raw.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3383e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folders created: ['data_raw', 'data_clean', 'slides', 'docs']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"data_raw\", \"data_clean\", \"slides\", \"docs\"]\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Project folders created:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5554d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"Test notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b029c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065c85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
