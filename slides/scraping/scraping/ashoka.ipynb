{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478fc5bb",
   "metadata": {},
   "source": [
    "Section 1: Project Setup and Ethical Scraping Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f568d",
   "metadata": {},
   "source": [
    "1.1. Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a2133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ee5958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical scraping setup complete.\n",
      "User-Agent set to: Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)\n",
      "Rate limiting is in effect. Requests will have a delay.\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Initial Setup and Ethical Practices\n",
    "\n",
    "# Import the core libraries for web scraping and data handling\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Define the base URL and headers for ethical scraping\n",
    "base_url = \"https://www.treasury.gov.lk\"\n",
    "headers = {\n",
    "    'User-Agent': 'Group 5 DA2009 Project Scraper/1.0 (Educational Purposes)'\n",
    "}\n",
    "\n",
    "# Add a polite delay before starting to be a good web citizen\n",
    "time.sleep(2)\n",
    "print(\"Ethical scraping setup complete.\")\n",
    "print(f\"User-Agent set to: {headers['User-Agent']}\")\n",
    "print(\"Rate limiting is in effect. Requests will have a delay.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968afd8",
   "metadata": {},
   "source": [
    "Section 2: Scraping Structured Data (HTML Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796a90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping for tables on the homepage...\n",
      "Successfully scraped the homepage and found 4 tables.\n",
      "Table 1 saved to 'table_1_homepage_data.csv'.\n",
      "Table 2 saved to 'table_2_homepage_data.csv'.\n",
      "Table 3 saved to 'table_3_homepage_data.csv'.\n",
      "Table 4 saved to 'table_4_homepage_data.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_25776\\875692028.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Scraping Structured Data (HTML Tables)\n",
    "\n",
    "# Define the URL for the homepage\n",
    "homepage_url = f\"{base_url}\"\n",
    "\n",
    "try:\n",
    "    print(\"\\nScraping for tables on the homepage...\")\n",
    "    response = requests.get(homepage_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "\n",
    "    if len(tables) > 0:\n",
    "        print(f\"Successfully scraped the homepage and found {len(tables)} tables.\")\n",
    "        \n",
    "        # Use a for loop to save each table to a uniquely named CSV file\n",
    "        for i, table in enumerate(tables):\n",
    "            filename = f'table_{i+1}_homepage_data.csv'\n",
    "            table.to_csv(filename, index=False)\n",
    "            print(f\"Table {i+1} saved to '{filename}'.\")\n",
    "    else:\n",
    "        print(\"No tables found on the homepage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while scraping tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413a0c3",
   "metadata": {},
   "source": [
    "Section 3: Scraping Unstructured Data (News & Publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77a5b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping news headlines and links...\n",
      "An error occurred while scraping news: 404 Client Error: Not Found for url: https://www.treasury.gov.lk/web\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Scraping Unstructured Data (News & Publications)\n",
    "\n",
    "# Define a URL for the news or publication page\n",
    "# You must find the correct URL by navigating the website\n",
    "news_url = f\"{base_url}/web/newsroom\" # This is a placeholder URL\n",
    "scraped_news = []\n",
    "\n",
    "try:\n",
    "    print(\"\\nScraping news headlines and links...\")\n",
    "    response = requests.get(news_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all news headlines. **You must inspect the page to find the correct tags and classes.**\n",
    "    news_items = soup.find_all('div', class_='news-item') # This is a placeholder class\n",
    "\n",
    "    if news_items:\n",
    "        for item in news_items:\n",
    "            # Extract the title and link from the HTML element\n",
    "            title_tag = item.find('a')\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = requests.compat.urljoin(base_url, title_tag['href'])\n",
    "                scraped_news.append({'Title': title, 'Link': link})\n",
    "        \n",
    "        df_news = pd.DataFrame(scraped_news)\n",
    "        print(\"Successfully scraped news data:\")\n",
    "        print(df_news.head())\n",
    "        df_news.to_csv('news_headlines.csv', index=False)\n",
    "        print(\"Data saved to 'news_headlines.csv'.\")\n",
    "    else:\n",
    "        print(\"No news items found with the specified class.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while scraping news: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6406e",
   "metadata": {},
   "source": [
    "Yes, here is a note you can add to your Jupyter Notebook. This note explains the error and tells your team members how to proceed, allowing them to continue working on the other sections of the project.\n",
    "\n",
    "Team Note: Status Update on News & Publications Scraping\n",
    "Issue: An error occurred while attempting to scrape the news and publications section. The script returned a 404 Client Error: Not Found, which means the URL we were using is no longer valid.\n",
    "\n",
    "Impact: This error prevents the script from scraping data from this specific section. This is a common challenge in web scraping because website URLs and content can change at any time without warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a89ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "section 4. Extracting Links to Publications & Circulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c444b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped publication links.\n",
      "                                     Text  \\\n",
      "0  Acts, Gazettes, Circulars & Guidelines   \n",
      "1                                Gazettes   \n",
      "2                                    Acts   \n",
      "3                               Circulars   \n",
      "4                              Guidelines   \n",
      "\n",
      "                                                 URL  \n",
      "0  https://www.treasury.gov.lk/acts-gazettes-circ...  \n",
      "1  https://www.treasury.gov.lk/acts-gazettes-circ...  \n",
      "2  https://www.treasury.gov.lk/web/circular-gazet...  \n",
      "3  https://www.treasury.gov.lk/web/circular-gazet...  \n",
      "4  https://www.treasury.gov.lk/web/circular-gazet...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.treasury.gov.lk\"\n",
    "headers = {'User-Agent': 'Group 5 DA2009 Project Scraper/1.0'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all links to circulars or publications\n",
    "    publication_links = soup.find_all('a', href=True)\n",
    "\n",
    "    scraped_links = []\n",
    "    for link in publication_links:\n",
    "        # Check if the link's URL or text contains a relevant keyword\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        if 'circular' in href.lower() or 'publication' in href.lower() or 'circular' in text.lower():\n",
    "            full_url = requests.compat.urljoin(url, href)\n",
    "            scraped_links.append({'Text': text, 'URL': full_url})\n",
    "\n",
    "    df_links = pd.DataFrame(scraped_links)\n",
    "    if not df_links.empty:\n",
    "        print(\"Successfully scraped publication links.\")\n",
    "        print(df_links.head())\n",
    "        df_links.to_csv('publication_links.csv', index=False)\n",
    "    else:\n",
    "        print(\"No publication links found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdff0e",
   "metadata": {},
   "source": [
    "section 5  Scraping Contact Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb17b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped contact information.\n",
      "                                        Contact Info\n",
      "0                                         Contact us\n",
      "1                                    Contact Details\n",
      "2                                         Contact us\n",
      "3  {\"props\":{\"pageProps\":{\"ua\":{\"browser\":\"Chrome...\n"
     ]
    }
   ],
   "source": [
    "# This assumes contact info is on the main page.\n",
    "# If not, you'd need to first find the 'Contact Us' link.\n",
    "\n",
    "# ... (previous code to get `soup` object)\n",
    "\n",
    "contacts = soup.find_all(string=re.compile(r'contact|email|phone', re.I))\n",
    "\n",
    "contact_data = []\n",
    "for contact in contacts:\n",
    "    parent = contact.parent.get_text(strip=True)\n",
    "    contact_data.append({'Contact Info': parent})\n",
    "\n",
    "df_contacts = pd.DataFrame(contact_data)\n",
    "if not df_contacts.empty:\n",
    "    print(\"Successfully scraped contact information.\")\n",
    "    print(df_contacts.head())\n",
    "    df_contacts.to_csv('contact_info.csv', index=False)\n",
    "else:\n",
    "    print(\"No contact information found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee7ceb",
   "metadata": {},
   "source": [
    "section 6  Getting the Page Title and Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9008982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page Title: Ministry of Finance - Sri lanka\n",
      "\n",
      "Page Headings:\n",
      "Ministry of Finance, Planning and Economic Development\n",
      "Press Release\n",
      "\n",
      "Monthly Fiscal Review Report\n",
      "Press Release\n",
      "Press Release\n",
      "Press Release\n",
      "Press Release\n",
      "\n",
      "Monthly Fiscal Review Report\n",
      "Press Release\n",
      "Press Release\n",
      "Press Release\n",
      "Press Release\n",
      "Economic Indicators\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code to get `soup` object)\n",
    "\n",
    "page_title = soup.title.get_text(strip=True) if soup.title else \"No Title Found\"\n",
    "print(f\"\\nPage Title: {page_title}\")\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "print(\"\\nPage Headings:\")\n",
    "for heading in headings:\n",
    "    print(heading.get_text(strip=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
